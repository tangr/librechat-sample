version: "1"  # 添加 version 字段

registration:
  socialLogins: ["google", "facebook", "github", "discord", "openid"]
  allowedDomains:
    - "gmail.com"
    - "protonmail.com"

endpoints:
  custom:
    - name: "LiteLLM"
      # A place holder - otherwise it becomes the default (OpenAI) key
      # Provide the key instead in each "model" block within "litellm/litellm-config.yaml"
      apiKey: "sk-from-config-file"
      # See the required changes above in "Start LiteLLM Proxy Server" step.
      baseURL: "http://host.docker.internal:4000"
      # A "default" model to start new users with. The "fetch" will pull the rest of the available models from LiteLLM
      # More or less this is "irrelevant", you can pick any model. Just pick one you have defined in LiteLLM.
      models:
        default:
          - "claude-3-7-sonnet-v1"
          - "claude-3-5-sonnet-v2"
          - "claude-3-5-haiku"
          # - "claude-3-5-sonnet"
          # - "claude-3-opus"
        fetch: false
      titleConvo: true
      titleMethod: functions
      titleModel: "claude-3-5-sonnet-v2"
      titleMessageRole: "user"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Lite LLM"
    - name: "Deepseek"
      apiKey: "${DEEPSEEK_API_KEY}"
      # baseURL: "https://api.deepseek.com/v1"
      baseURL: "${DEEPSEEK_API_ENDPOINT}"
      models:
        default: ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]
        fetch: false
      titleConvo: true
      titleModel: "current_model"
      titleMessageRole: "system"
      modelDisplayLabel: "Deepseek"
